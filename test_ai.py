#!/usr/bin/env python3
"""
Tests b√°sicos para el m√≥dulo de IA
Dashboard IT - Cl√≠nica Bonsana
"""

import os
import sys
import unittest
import pandas as pd
import tempfile
import json
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

# Agregar directorio ra√≠z al path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Importar m√≥dulos de IA
try:
    from ai.config import AIConfig, AnalysisType
    from ai.gemini_client import GeminiClient
    from ai.analyzer import AIAnalyzer
    from ai.prompts import PromptManager
    from ai.utils import DataValidator, CacheManager, ResponseFormatter, MetricsCalculator
except ImportError as e:
    print(f"Error al importar m√≥dulos de IA: {e}")
    print("Aseg√∫rate de que el m√≥dulo ai/ est√© disponible")
    sys.exit(1)

class TestAIConfig(unittest.TestCase):
    """Tests para configuraci√≥n de IA"""
    
    def test_config_creation(self):
        """Test creaci√≥n de configuraci√≥n"""
        config = AIConfig(
            api_key="test_key",
            model_name="gemini-2.0-flash-exp"
        )
        
        self.assertEqual(config.api_key, "test_key")
        self.assertEqual(config.model_name, "gemini-2.0-flash-exp")
        self.assertEqual(config.max_csv_rows, 1000)
    
    def test_config_validation(self):
        """Test validaci√≥n de configuraci√≥n"""
        # Configuraci√≥n inv√°lida
        config = AIConfig(api_key="", model_name="invalid_model")
        errors = config.validate()
        
        self.assertTrue(len(errors) > 0)
        self.assertIn('api_key', errors)
        self.assertIn('model', errors)
        
        # Configuraci√≥n v√°lida
        config = AIConfig(
            api_key="AIzaSyTestKey123",
            model_name="gemini-2.0-flash-exp"
        )
        errors = config.validate()
        
        # Puede tener errores de directorio, pero no de api_key o model
        if errors:
            self.assertNotIn('api_key', errors)
            self.assertNotIn('model', errors)

class TestDataValidator(unittest.TestCase):
    """Tests para validador de datos"""
    
    def setUp(self):
        """Configurar datos de test"""
        self.valid_data = {
            'ID': ['001', '002', '003'],
            'T√≠tulo': ['Test 1', 'Test 2', 'Test 3'],
            'Tipo': ['Incidencia', 'Requerimiento', 'Incidencia'],
            'Estado': ['Resueltas', 'Nuevo', 'En curso (asignada)'],
            'Fecha de Apertura': ['2024-01-01 10:00', '2024-01-02 11:00', '2024-01-03 12:00']
        }
        self.df_valid = pd.DataFrame(self.valid_data)
    
    def test_validate_valid_structure(self):
        """Test validaci√≥n de estructura v√°lida"""
        result = DataValidator.validate_csv_structure(self.df_valid)
        
        self.assertTrue(result['valid'])
        self.assertEqual(len(result['errors']), 0)
        self.assertEqual(result['stats']['total_rows'], 3)
    
    def test_validate_missing_columns(self):
        """Test validaci√≥n con columnas faltantes"""
        df_invalid = pd.DataFrame({
            'ID': ['001', '002'],
            'T√≠tulo': ['Test 1', 'Test 2']
            # Faltan columnas requeridas
        })
        
        result = DataValidator.validate_csv_structure(df_invalid)
        
        self.assertFalse(result['valid'])
        self.assertTrue(len(result['errors']) > 0)
    
    def test_validate_empty_dataframe(self):
        """Test validaci√≥n de DataFrame vac√≠o"""
        df_empty = pd.DataFrame()
        
        result = DataValidator.validate_csv_structure(df_empty)
        
        self.assertFalse(result['valid'])
        self.assertTrue(any('vac√≠o' in error for error in result['errors']))
    
    def test_clean_text_data(self):
        """Test limpieza de texto"""
        dirty_text = "Texto con ‚ô†‚ô£‚ô¶‚ô• caracteres    especiales\n\n\t"
        clean_text = DataValidator.clean_text_data(dirty_text)
        
        self.assertNotIn('‚ô†', clean_text)
        self.assertNotIn('\n', clean_text)
        self.assertEqual(clean_text.strip(), clean_text)

class TestCacheManager(unittest.TestCase):
    """Tests para gestor de cache"""
    
    def setUp(self):
        """Configurar cache temporal"""
        self.temp_dir = tempfile.mkdtemp()
        self.cache_manager = CacheManager(self.temp_dir)
    
    def tearDown(self):
        """Limpiar cache temporal"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_cache_set_get(self):
        """Test guardar y obtener cache"""
        test_data = {"test": "data", "number": 123}
        
        # Guardar en cache
        success = self.cache_manager.set("test_key", test_data)
        self.assertTrue(success)
        
        # Obtener del cache
        cached_data = self.cache_manager.get("test_key")
        self.assertIsNotNone(cached_data)
        self.assertEqual(cached_data["test"], "data")
        self.assertEqual(cached_data["number"], 123)
    
    def test_cache_miss(self):
        """Test cache miss"""
        result = self.cache_manager.get("nonexistent_key")
        self.assertIsNone(result)

class TestResponseFormatter(unittest.TestCase):
    """Tests para formateador de respuestas"""
    
    def test_format_markdown_response(self):
        """Test formateo de markdown"""
        markdown_text = "# Header\nSome text\n## Subheader\nMore text"
        formatted = ResponseFormatter.format_markdown_response(markdown_text)
        
        self.assertIn("# Header", formatted)
        self.assertIn("## Subheader", formatted)
    
    def test_extract_sections(self):
        """Test extracci√≥n de secciones"""
        text = """
# Analisis Principal
Contenido del an√°lisis

## Resumen Ejecutivo
Resumen importante

### Detalles
Informaci√≥n detallada
"""
        sections = ResponseFormatter.extract_sections(text)
        
        self.assertIn('analisis_principal', sections)
        self.assertIn('resumen_ejecutivo', sections)
        self.assertIn('detalles', sections)
    
    def test_generate_summary(self):
        """Test generaci√≥n de resumen"""
        long_text = "Primera oraci√≥n. Segunda oraci√≥n. Tercera oraci√≥n. Cuarta oraci√≥n."
        summary = ResponseFormatter.generate_summary(long_text, max_sentences=2)
        
        # Deber√≠a contener las primeras oraciones
        self.assertIn("Primera", summary)
        self.assertIn("Segunda", summary)

class TestMetricsCalculator(unittest.TestCase):
    """Tests para calculadora de m√©tricas"""
    
    def test_calculate_efficiency_score(self):
        """Test c√°lculo de score de eficiencia"""
        metrics = {
            'resolution_rate': 95.0,
            'sla_compliance': 98.0,
            'avg_resolution_time_hours': 6.0
        }
        
        score = MetricsCalculator.calculate_efficiency_score(metrics)
        
        self.assertGreaterEqual(score, 0)
        self.assertLessEqual(score, 100)
        self.assertIsInstance(score, float)
    
    def test_calculate_trend(self):
        """Test c√°lculo de tendencias"""
        # Tendencia creciente
        values_up = [10, 12, 15, 18, 20]
        trend, change = MetricsCalculator.calculate_trend(values_up)
        
        self.assertEqual(trend, "creciente")
        self.assertGreater(change, 0)
        
        # Tendencia decreciente
        values_down = [20, 18, 15, 12, 10]
        trend, change = MetricsCalculator.calculate_trend(values_down)
        
        self.assertEqual(trend, "decreciente")
        self.assertLess(change, 0)
        
        # Tendencia estable
        values_stable = [10, 10.2, 9.8, 10.1, 10]
        trend, change = MetricsCalculator.calculate_trend(values_stable)
        
        self.assertEqual(trend, "estable")

class TestPromptManager(unittest.TestCase):
    """Tests para gestor de prompts"""
    
    def test_get_comprehensive_analysis_prompt(self):
        """Test obtener prompt de an√°lisis completo"""
        prompt = PromptManager.get_comprehensive_analysis_prompt()
        
        self.assertIsInstance(prompt, str)
        self.assertIn("Cl√≠nica Bonsana", prompt)
        self.assertIn("an√°lisis exhaustivo", prompt)
        self.assertGreater(len(prompt), 500)  # Debe ser un prompt sustancial
    
    def test_get_quick_analysis_prompt(self):
        """Test obtener prompt de an√°lisis r√°pido"""
        prompt = PromptManager.get_quick_analysis_prompt()
        
        self.assertIsInstance(prompt, str)
        self.assertIn("an√°lisis r√°pido", prompt)
        self.assertIn("500 palabras", prompt)
    
    def test_get_available_prompts(self):
        """Test obtener prompts disponibles"""
        prompts = PromptManager.get_available_prompts()
        
        self.assertIsInstance(prompts, dict)
        self.assertIn("comprehensive", prompts)
        self.assertIn("quick", prompts)
        self.assertIn("sla", prompts)
    
    def test_get_custom_prompt(self):
        """Test obtener prompt personalizado"""
        focus_area = "An√°lisis de rendimiento"
        questions = ["¬øCu√°l es la eficiencia?", "¬øQu√© mejoras recomiendas?"]
        
        prompt = PromptManager.get_custom_prompt(focus_area, questions)
        
        self.assertIn(focus_area, prompt)
        self.assertIn(questions[0], prompt)
        self.assertIn(questions[1], prompt)

class TestGeminiClientMocked(unittest.TestCase):
    """Tests para cliente Gemini (mocked)"""
    
    @patch('google.generativeai.configure')
    @patch('google.generativeai.GenerativeModel')
    def test_gemini_client_init(self, mock_model, mock_configure):
        """Test inicializaci√≥n del cliente"""
        mock_model_instance = Mock()
        mock_model.return_value = mock_model_instance
        
        client = GeminiClient(api_key="test_key")
        
        mock_configure.assert_called_once_with(api_key="test_key")
        mock_model.assert_called_once()
        self.assertEqual(client.api_key, "test_key")
    
    def test_prepare_csv_data(self):
        """Test preparaci√≥n de datos CSV"""
        # Crear archivo CSV temporal
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            f.write("ID;Tipo;Estado\n001;Incidencia;Resueltas\n002;Requerimiento;Nuevo\n")
            temp_csv = f.name
        
        try:
            client = GeminiClient(api_key="test_key")
            formatted_data = client.prepare_csv_data(temp_csv)
            
            self.assertIsInstance(formatted_data, str)
            self.assertIn("INFORMACI√ìN DEL DATASET", formatted_data)
            self.assertIn("ID", formatted_data)
            self.assertIn("Tipo", formatted_data)
            
        finally:
            os.unlink(temp_csv)

def run_integration_tests():
    """Ejecuta tests de integraci√≥n b√°sicos"""
    print("\nüß™ Ejecutando tests de integraci√≥n...")
    
    # Test 1: Verificar estructura de directorios
    print("   üìÅ Verificando estructura de directorios...")
    required_dirs = ['ai', 'data', 'templates']
    missing_dirs = [d for d in required_dirs if not os.path.exists(d)]
    
    if missing_dirs:
        print(f"   ‚ùå Directorios faltantes: {missing_dirs}")
        return False
    else:
        print("   ‚úÖ Estructura de directorios correcta")
    
    # Test 2: Verificar archivos del m√≥dulo de IA
    print("   üìÑ Verificando archivos del m√≥dulo de IA...")
    required_files = [
        'ai/__init__.py',
        'ai/config.py',
        'ai/gemini_client.py',
        'ai/analyzer.py',
        'ai/prompts.py',
        'ai/utils.py'
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print(f"   ‚ùå Archivos faltantes: {missing_files}")
        return False
    else:
        print("   ‚úÖ Archivos del m√≥dulo de IA presentes")
    
    # Test 3: Verificar importaciones
    print("   üì¶ Verificando importaciones...")
    try:
        from ai.config import AIConfig
        from ai.analyzer import AIAnalyzer
        from ai.prompts import PromptManager
        print("   ‚úÖ Importaciones exitosas")
    except ImportError as e:
        print(f"   ‚ùå Error de importaci√≥n: {e}")
        return False
    
    # Test 4: Verificar configuraci√≥n
    print("   ‚öôÔ∏è Verificando configuraci√≥n...")
    try:
        config = AIConfig.from_env()
        if not config.api_key:
            print("   ‚ö†Ô∏è  API key no configurada")
        else:
            print("   ‚úÖ Configuraci√≥n cargada")
    except Exception as e:
        print(f"   ‚ùå Error en configuraci√≥n: {e}")
        return False
    
    print("   ‚úÖ Tests de integraci√≥n completados")
    return True

def main():
    """Funci√≥n principal de tests"""
    print("="*60)
    print("    TESTS DEL M√ìDULO DE IA")
    print("    Dashboard IT - Cl√≠nica Bonsana")
    print("="*60)
    
    # Tests unitarios
    print("\nüî¨ Ejecutando tests unitarios...")
    
    # Crear suite de tests
    test_suite = unittest.TestSuite()
    
    # Agregar clases de test
    test_classes = [
        TestAIConfig,
        TestDataValidator,
        TestCacheManager,
        TestResponseFormatter,
        TestMetricsCalculator,
        TestPromptManager,
        TestGeminiClientMocked
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # Ejecutar tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # Tests de integraci√≥n
    integration_success = run_integration_tests()
    
    # Resumen
    print("\n" + "="*60)
    print("RESUMEN DE TESTS")
    print("="*60)
    
    unit_tests_passed = result.wasSuccessful()
    
    print(f"Tests unitarios: {'‚úÖ PASARON' if unit_tests_passed else '‚ùå FALLARON'}")
    print(f"Tests de integraci√≥n: {'‚úÖ PASARON' if integration_success else '‚ùå FALLARON'}")
    
    if result.failures:
        print(f"\nFallas: {len(result.failures)}")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback.split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nErrores: {len(result.errors)}")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback.split('Exception:')[-1].strip()}")
    
    overall_success = unit_tests_passed and integration_success
    
    print(f"\n{'üéâ TODOS LOS TESTS PASARON' if overall_success else '‚ö†Ô∏è ALGUNOS TESTS FALLARON'}")
    
    return 0 if overall_success else 1

if __name__ == '__main__':
    sys.exit(main())